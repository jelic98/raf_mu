# -*- coding: utf-8 -*-
"""MU - Zadatak 4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HXGvNAIgHbRxgIg0Lmze__A_ooVoLTYj
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x

!pip install nltk

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
import csv
import html
import re
from string import ascii_lowercase
import nltk
from nltk.tokenize import regexp_tokenize
from nltk.corpus import stopwords
from nltk import FreqDist
from sklearn.metrics import confusion_matrix, accuracy_score
import random

nltk.download('punkt')
nltk.download('stopwords')

data_x = []
data_y = []

# Ucitavanje podataka
print("Loading data...")
filename = '/content/drive/My Drive/Colab Notebooks/data/twitter.csv'
with open(filename, 'r', encoding='latin1') as f:
    reader = csv.reader(f, delimiter=',')
    next(reader, None)
    for row in reader:
        data_y.append(int(row[1]))
        data_x.append(row[2])

# Ciscenje podataka
print("Cleaning data...")
data_x = [html.unescape(x) for x in data_x]
data_x = [re.sub(r'https?://(?:[-\w.]|(?:%[\da-fA-F]{2}))+', '', x) for x in data_x]
data_x = [re.sub(r'[^\w\s]|\d+', '', x) for x in data_x]
data_x = [re.sub(r'\s\s+', ' ', x) for x in data_x]
data_x = [x.strip().lower() for x in data_x]
for c in ascii_lowercase:
    data_x = [re.sub(c + '{3,}', c+c, x) for x in data_x]
data_x = [regexp_tokenize(x, '\w+') for x in data_x]
stops = set(stopwords.words('english'))
data_x = [[w for w in x if not w in stops] for x in data_x]

nb_samples = 5000
data_x = data_x[:nb_samples]
data_y = data_y[:nb_samples]

# Racunanje BOW reprezentacije i LR metrike
print("Calculating BOW representation and LR metric...")
nb_words = 10000
freq = FreqDist([w for x in data_x for w in x])
vocab, _ = zip(*freq.most_common(nb_words))
nb_words = min(nb_words, len(vocab))
vec_x = np.zeros((len(data_x), len(vocab)), dtype=np.float32)
lr = np.zeros(nb_words, dtype=np.float32)
for j, w in enumerate(vocab):
    neg = 0
    pos = 0
    for i, x in enumerate(data_x):
        cnt = x.count(w)
        vec_x[i][j] = cnt
        neg += cnt if data_y[i] == 0 else 0
        pos += cnt if data_y[i] == 1 else 0
    lr[j] = (pos / neg) if pos >= 10 and neg >= 10 else 0

# Deljenje podataka na skup za treniranje i testiranje
print("Dividing dataset...")
train_x, test_x = np.split(vec_x, [int(len(vec_x)*0.8)])
train_y, test_y = np.split(data_y, [int(len(data_y)*0.8)])
nb_train = len(train_x)
nb_test = len(test_x)

# Racunanje broja pojavljivanja svake reci u svakoj klasi
print("Calculating every word occurrence for every class...")
nb_classes = 2
occs = np.zeros((nb_classes, nb_words), dtype=np.float32)
for i, y in enumerate(train_y):
    for w in range(nb_words):
        occs[y][w] += train_x[i][w]

# Racunanje P(rec|klasa)
print("Calculating P(word|class)...")
like = np.zeros((nb_classes, nb_words), dtype=np.float32)
nb_alpha = 1
for c in range(nb_classes):
    for w in range(nb_words):
        up = occs[c][w] + nb_alpha
        down = np.sum(occs[c]) + nb_words * nb_alpha
        like[c][w] = up / down

# Racunanje P(klasa|test)
print("Calculating P(class|test)...")
hyps = []
acts = []
priors = np.bincount(train_y) / nb_samples
class_trans = {'Negative': 0, 'Positive': 1, 0: 'Negative', 1: 'Positive'}
for i, x in enumerate(test_x):
    probs = np.zeros(nb_classes)
    for c in range(nb_classes):
        prob = np.log(priors[c])
        for w in range(nb_words):
            prob += x[w] * np.log(like[c][w])
        probs[c] = prob
    hyp_val = np.argmax(probs)
    act_val = test_y[i]
    match = hyp_val == act_val
    hyps.append(class_trans[hyp_val])
    acts.append(class_trans[act_val])
    if i % 10 == 0:
        print('{}/{} Predicted: {} Actual: {} Match: {}'
        .format(i+1, nb_test, class_trans[hyp_val], class_trans[act_val], match))

# Racunanje tacnosti modela
accuracy_score(acts, hyps)
print('Accuracy:', accuracy)

# Prikazivanje matrice konfuzije
cm = confusion_matrix(acts, hyps)
plt.matshow(cm)
plt.colorbar()
plt.xlabel('Hypothesis')
plt.ylabel('Actual')
plt.show()

# Pronalazenje pet najcesce koriscenih reci u negativnim tvitovima
freq_neg = FreqDist([w for i, x in enumerate(data_x) for w in x if data_y[i] == 0])
top_neg, _ = zip(*freq_neg.most_common(5))
print('Top negative:', top_neg)

# Pronalazenje pet najcesce koriscenih reci u pozitivnim tvitovima
freq_pos = FreqDist([w for i, x in enumerate(data_x) for w in x if data_y[i] == 1])
top_pos, _ = zip(*freq_pos.most_common(5))
print('Top positive:', top_pos)

# Pronalazenje pet reci sa najmanjom vrednoscu LR metrike
lr_min = []
min_cnt = 1
for i in lr.argsort():
    if min_cnt > 5:
        break
    if lr[i] > 0:
        lr_min.append(i)
        min_cnt += 1
print('LR lowest:', [vocab[x] for x in lr_min])

# Pronalazenje pet reci sa najvecom vrednoscu LR metrike
lr_max = []
max_cnt = 1
for i in (-lr).argsort():
    if max_cnt > 5:
        break
    if lr[i] > 0:
        lr_max.append(i)
        max_cnt += 1
print('LR highest:', [vocab[x] for x in lr_max])