# -*- coding: utf-8 -*-
"""MU - Zadatak 4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HXGvNAIgHbRxgIg0Lmze__A_ooVoLTYj
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x

!pip install nltk

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
import csv
import html
import re
from string import ascii_lowercase
import nltk
from nltk.tokenize import regexp_tokenize
from nltk.corpus import stopwords
from nltk import FreqDist
import pickle
from sklearn.metrics import confusion_matrix, accuracy_score
import random

nltk.download('punkt')
nltk.download('stopwords')

filename = '/content/drive/My Drive/Colab Notebooks/data/twitter.csv'
pklname = '/content/drive/My Drive/Colab Notebooks/data/twitter_100k.pkl'
nb_samples = 100000
nb_words = 10000
nb_classes = 2
nb_alpha = 1
ds_from_cache = True

class Dataset:
    def __init__(self, filename):
        self.data_x = []
        self.data_y = []
        self.load(filename)
        self.clean()
        self.calculate_bow_lr()
        self.split()
        self.calculate_occurrences()
        self.calculate_likelihoods()

    # Ucitavanje podataka
    def load(self, filename):
        print('Loading data...')

        with open(filename, 'r', encoding='latin1') as fin:
            reader = csv.reader(fin, delimiter=',')
            next(reader, None)
            for row in reader:
                self.data_y.append(int(row[1]))
                self.data_x.append(row[2])

    # Ciscenje podataka
    def clean(self):
        print('Cleaning data...')

        self.data_x = [html.unescape(x) for x in self.data_x]

        self.data_x = [re.sub(r'https?://(?:[-\w.]|(?:%[\da-fA-F]{2}))+', '', x) for x in self.data_x]
        self.data_x = [re.sub(r'[^\w\s]|\d+', '', x) for x in self.data_x]
        self.data_x = [re.sub(r'\s\s+', ' ', x) for x in self.data_x]
        self.data_x = [x.strip().lower() for x in self.data_x]
        for c in ascii_lowercase:
            self.data_x = [re.sub(c + '{3,}', c+c, x) for x in self.data_x]
        
        self.data_x = [regexp_tokenize(x, '\w+') for x in self.data_x]
        
        stops = set(stopwords.words('english'))
        self.data_x = [[w for w in x if not w in stops] for x in self.data_x]

        self.data_x = self.data_x[:nb_samples]
        self.data_y = self.data_y[:nb_samples]

    def update_nb_words(self):
        global nb_words
        nb_words = min(nb_words, len(self.vocab))

    # Racunanje BOW reprezentacije i LR metrike
    def calculate_bow_lr(self):
        print('Calculating BOW representation and LR metric...')

        global nb_words

        freq = FreqDist([w for x in self.data_x for w in x])
        self.vocab, _ = zip(*freq.most_common(nb_words))
        self.update_nb_words()
        
        self.vec_x = np.zeros((len(self.data_x), len(self.vocab)), dtype=np.float32)
        self.lr = np.zeros(nb_words, dtype=np.float32)

        for j, w in enumerate(self.vocab):
            neg = 0
            pos = 0
            for i, x in enumerate(self.data_x):
                cnt = x.count(w)
                self.vec_x[i][j] = cnt
                if self.data_y[i] == 0:
                    neg += cnt
                else:
                    pos += cnt
            if pos >= 10 and neg >= 10:
                self.lr[j] = pos / neg
            if j % 100 == 0:
                print('[calculate_bow_lr] Word: {}/{}'.format(j, nb_words))

    # Deljenje podataka na skup za treniranje i testiranje
    def split(self):
        print('Splitting data...')

        self.train_x, self.test_x = np.split(self.vec_x, [int(len(self.vec_x)*0.8)])
        self.train_y, self.test_y = np.split(self.data_y, [int(len(self.data_y)*0.8)])
        
        self.nb_train = len(self.train_x)
        self.nb_test = len(self.test_x)

    # Racunanje broja pojavljivanja svake reci u svakoj klasi
    def calculate_occurrences(self):
        print('Calculating every word occurrence for every class...')

        global nb_words, nb_classes
        
        self.occs = np.zeros((nb_classes, nb_words), dtype=np.float32)
        for i, y in enumerate(self.train_y):
            for w in range(nb_words):
                self.occs[y][w] += self.train_x[i][w]
            if i % 1000 == 0:
                print('[calculate_occurrences] Object: {}/{}'.format(i, self.nb_train))

    # Racunanje P(rec|klasa)
    def calculate_likelihoods(self):
        print('Calculating P(word|class)...')

        global nb_words, nb_classes, nb_alpha

        self.like = np.zeros((nb_classes, nb_words), dtype=np.float32)
        for c in range(nb_classes):
            for w in range(nb_words):
                up = self.occs[c][w] + nb_alpha
                down = np.sum(self.occs[c]) + nb_words * nb_alpha
                self.like[c][w] = up / down
                if w % 1000 == 0:
                    print('[calculate_likelihoods] Word: {}/{}'.format(w, nb_words))

if ds_from_cache:
    with open(pklname, 'rb') as pkl:
        print('Using cached dataset...')
        ds = pickle.load(pkl)
        ds.update_nb_words()
else:
    with open(pklname, 'wb') as pkl:
        print('Using new dataset...')
        ds = Dataset(filename)
        print('Serializing new dataset...')
        pickle.dump(ds, pkl, pickle.HIGHEST_PROTOCOL)

# Racunanje P(klasa|test)
print('Calculating P(class|test)...')
hyps = []
acts = []
priors = np.bincount(ds.train_y) / nb_samples
class_trans = {'Negative': 0, 'Positive': 1, 0: 'Negative', 1: 'Positive'}
for i, x in enumerate(ds.test_x):
    probs = np.zeros(nb_classes)
    for c in range(nb_classes):
        prob = np.log(priors[c])
        for w in range(nb_words):
            prob += x[w] * np.log(ds.like[c][w])
        probs[c] = prob
    hyp_val = np.argmax(probs)
    act_val = ds.test_y[i]
    match = hyp_val == act_val
    hyps.append(class_trans[hyp_val])
    acts.append(class_trans[act_val])
    if i % 100 == 0:
        print('{}/{} Predicted: {} Actual: {} Match: {}'
        .format(i+1, ds.nb_test, class_trans[hyp_val], class_trans[act_val], match))

# Racunanje tacnosti modela
cm = confusion_matrix(acts, hyps)
tn, _, _, tp = cm.ravel()
acc = (tn + tp) / ds.nb_test
print('Accuracy:', acc)

# Prikazivanje matrice konfuzije
plt.matshow(cm)
plt.colorbar()
plt.xlabel('Hypothesis')
plt.ylabel('Actual')
plt.show()

# Pronalazenje pet najcesce koriscenih reci u negativnim tvitovima
freq_neg = FreqDist([w for i, x in enumerate(ds.data_x) for w in x if ds.data_y[i] == 0])
top_neg, _ = zip(*freq_neg.most_common(5))
print('Top negative:', top_neg)

# Pronalazenje pet najcesce koriscenih reci u pozitivnim tvitovima
freq_pos = FreqDist([w for i, x in enumerate(ds.data_x) for w in x if ds.data_y[i] == 1])
top_pos, _ = zip(*freq_pos.most_common(5))
print('Top positive:', top_pos)

# Pronalazenje pet reci sa najmanjom vrednoscu LR metrike
lr_min = []
min_cnt = 1
for i in ds.lr.argsort():
    if min_cnt > 5:
        break
    if ds.lr[i] > 0:
        lr_min.append(i)
        min_cnt += 1
print('LR lowest:', [ds.vocab[x] for x in lr_min])

# Pronalazenje pet reci sa najvecom vrednoscu LR metrike
lr_max = []
max_cnt = 1
for i in (-ds.lr).argsort():
    if max_cnt > 5:
        break
    if ds.lr[i] > 0:
        lr_max.append(i)
        max_cnt += 1
print('LR highest:', [ds.vocab[x] for x in lr_max])